<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="assets/xml/rss.xsl" media="all"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>绿萝间</title><link>http://muxuezi.github.io/</link><description>Tao Junjie blog</description><atom:link href="http://muxuezi.github.io/rss.xml" type="application/rss+xml" rel="self"></atom:link><language>en</language><lastBuildDate>Thu, 02 Jul 2015 01:41:34 GMT</lastBuildDate><generator>http://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>7-dimensionality-reduction-with-pca</title><link>http://muxuezi.github.io/posts/7-dimensionality-reduction-with-pca.html</link><dc:creator>tj2</dc:creator><description>&lt;div&gt;&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="用PCA降维"&gt;用PCA降维&lt;a class="anchor-link" href="http://muxuezi.github.io/posts/7-dimensionality-reduction-with-pca.html#%E7%94%A8PCA%E9%99%8D%E7%BB%B4"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;本章我们将介绍一种降维方法，PCA（Principal Component Analysis，主成分分析）。降维致力于解决三类问题。第一，降维可以缓解维度灾难问题。第二，降维可以在压缩数据的同时让信息损失最小化。第三，理解几百个维度的数据结构很困难，两三个维度的数据通过可视化更容易理解。下面，我们用PCA将一个高维数据降成二维，方便可视化，之后，我们建一个脸部识别系统。&lt;/p&gt;
&lt;p&gt;&lt;a href="http://muxuezi.github.io/posts/7-dimensionality-reduction-with-pca.html"&gt;Read more…&lt;/a&gt; (7 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>CHS</category><category>ipython</category><category>Machine Learning</category><category>Python</category><guid>http://muxuezi.github.io/posts/7-dimensionality-reduction-with-pca.html</guid><pubDate>Thu, 02 Jul 2015 01:38:42 GMT</pubDate></item><item><title>name-and-dream</title><link>http://muxuezi.github.io/posts/name-and-dream.html</link><dc:creator>tj2</dc:creator><description>&lt;p&gt;&lt;a href="http://muxuezi.github.io/posts/name-and-dream.html"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;</description><guid>http://muxuezi.github.io/posts/name-and-dream.html</guid><pubDate>Wed, 01 Jul 2015 16:00:00 GMT</pubDate></item><item><title>双色球2015075期(2015-06-30)数据分析报告</title><link>http://muxuezi.github.io/posts/slott-2015075-2015-06-30-report.html</link><dc:creator>tj2</dc:creator><description>&lt;div&gt;&lt;p&gt;如有雷同，纯属巧合&lt;/p&gt;
&lt;p&gt;&lt;a href="http://muxuezi.github.io/posts/slott-2015075-2015-06-30-report.html"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Lottery</category><guid>http://muxuezi.github.io/posts/slott-2015075-2015-06-30-report.html</guid><pubDate>Wed, 01 Jul 2015 00:00:00 GMT</pubDate></item><item><title>大乐透15074期(2015-06-29)数据分析报告</title><link>http://muxuezi.github.io/posts/dlott-15074-2015-06-29-report.html</link><dc:creator>tj2</dc:creator><description>&lt;div&gt;&lt;p&gt;如有雷同，纯属巧合&lt;/p&gt;
&lt;p&gt;&lt;a href="http://muxuezi.github.io/posts/dlott-15074-2015-06-29-report.html"&gt;Read more…&lt;/a&gt; (6 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Lottery</category><guid>http://muxuezi.github.io/posts/dlott-15074-2015-06-29-report.html</guid><pubDate>Tue, 30 Jun 2015 00:00:00 GMT</pubDate></item><item><title>4-from-linear-regression-to-logistic-regression</title><link>http://muxuezi.github.io/posts/4-from-linear-regression-to-logistic-regression.html</link><dc:creator>tj2</dc:creator><description>&lt;div&gt;&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="从线性回归到逻辑回归"&gt;从线性回归到逻辑回归&lt;a class="anchor-link" href="http://muxuezi.github.io/posts/4-from-linear-regression-to-logistic-regression.html#%E4%BB%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%88%B0%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;在第2章，线性回归里面，我们介绍了一元线性回归，多元线性回归和多项式回归。这些模型都是广义线性回归模型的具体形式，广义线性回归是一种灵活的框架，比普通线性回归要求更少的假设。这一章，我们讨论广义线性回归模型的具体形式的另一种形式，逻辑回归（logistic regression）。&lt;/p&gt;
&lt;p&gt;&lt;a href="http://muxuezi.github.io/posts/4-from-linear-regression-to-logistic-regression.html"&gt;Read more…&lt;/a&gt; (9 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>CHS</category><category>ipython</category><category>Machine Learning</category><category>Python</category><guid>http://muxuezi.github.io/posts/4-from-linear-regression-to-logistic-regression.html</guid><pubDate>Mon, 29 Jun 2015 12:49:07 GMT</pubDate></item><item><title>6-clustering-with-k-means</title><link>http://muxuezi.github.io/posts/6-clustering-with-k-means.html</link><dc:creator>tj2</dc:creator><description>&lt;div&gt;&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="K-Means聚类"&gt;K-Means聚类&lt;a class="anchor-link" href="http://muxuezi.github.io/posts/6-clustering-with-k-means.html#K-Means%E8%81%9A%E7%B1%BB"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;前面几章我们介绍了监督学习，包括从带标签的数据中学习的回归和分类算法。本章，我们讨论无监督学习算法，聚类（clustering）。聚类是用于找出不带标签数据的相似性的算法。我们将介绍K-Means聚类思想，解决一个图像压缩问题，然后对算法的效果进行评估。最后，我们把聚类和分类算法组合起来，解决一个半监督学习问题。&lt;/p&gt;
&lt;p&gt;&lt;a href="http://muxuezi.github.io/posts/6-clustering-with-k-means.html"&gt;Read more…&lt;/a&gt; (10 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>CHS</category><category>ipython</category><category>Machine Learning</category><category>Python</category><guid>http://muxuezi.github.io/posts/6-clustering-with-k-means.html</guid><pubDate>Mon, 29 Jun 2015 12:29:49 GMT</pubDate></item><item><title>5-nonlinear-classification-and-regression-with-decision-trees</title><link>http://muxuezi.github.io/posts/5-nonlinear-classification-and-regression-with-decision-trees.html</link><dc:creator>tj2</dc:creator><description>&lt;div&gt;&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;h2 id="决策树——非线性回归与分类"&gt;决策树——非线性回归与分类&lt;a class="anchor-link" href="http://muxuezi.github.io/posts/5-nonlinear-classification-and-regression-with-decision-trees.html#%E5%86%B3%E7%AD%96%E6%A0%91%E2%80%94%E2%80%94%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E5%88%86%E7%B1%BB"&gt;¶&lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class="cell border-box-sizing text_cell rendered"&gt;
&lt;div class="prompt input_prompt"&gt;
&lt;/div&gt;
&lt;div class="inner_cell"&gt;
&lt;div class="text_cell_render border-box-sizing rendered_html"&gt;
&lt;p&gt;前面几章，我们介绍的模型都是广义线性模型，基本方法都是通过联接方程构建解释变量与若干响应变量的关联关系。我们用多元线性回归解决回归问题，逻辑回归解决分类问题。本章我们要讨论一种简单的非线性模型，用来解决回归与分类问题，称为决策树（decision tree）。首先，我们将用决策树做一个广告屏蔽器，可以将网页中的广告内容屏蔽掉。之后，我们介绍集成学习（lensemble learning）方法，通过将一系列学习方法集成使用，以取得更好的训练效果。&lt;/p&gt;
&lt;p&gt;&lt;a href="http://muxuezi.github.io/posts/5-nonlinear-classification-and-regression-with-decision-trees.html"&gt;Read more…&lt;/a&gt; (6 min remaining to read)&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</description><category>CHS</category><category>ipython</category><category>Machine Learning</category><category>Python</category><guid>http://muxuezi.github.io/posts/5-nonlinear-classification-and-regression-with-decision-trees.html</guid><pubDate>Mon, 29 Jun 2015 12:29:22 GMT</pubDate></item><item><title>双色球2015074期(2015-06-28)数据分析报告</title><link>http://muxuezi.github.io/posts/slott-2015074-2015-06-28-report.html</link><dc:creator>tj2</dc:creator><description>&lt;div&gt;&lt;p&gt;如有雷同，纯属巧合&lt;/p&gt;
&lt;p&gt;&lt;a href="http://muxuezi.github.io/posts/slott-2015074-2015-06-28-report.html"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Lottery</category><guid>http://muxuezi.github.io/posts/slott-2015074-2015-06-28-report.html</guid><pubDate>Mon, 29 Jun 2015 00:00:00 GMT</pubDate></item><item><title>大乐透15073期(2015-06-27)数据分析报告</title><link>http://muxuezi.github.io/posts/dlott-15073-2015-06-27-report.html</link><dc:creator>tj2</dc:creator><description>&lt;div&gt;&lt;p&gt;如有雷同，纯属巧合&lt;/p&gt;
&lt;p&gt;&lt;a href="http://muxuezi.github.io/posts/dlott-15073-2015-06-27-report.html"&gt;Read more…&lt;/a&gt; (6 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Lottery</category><guid>http://muxuezi.github.io/posts/dlott-15073-2015-06-27-report.html</guid><pubDate>Sun, 28 Jun 2015 00:00:00 GMT</pubDate></item><item><title>双色球2015073期(2015-06-25)数据分析报告</title><link>http://muxuezi.github.io/posts/slott-2015073-2015-06-25-report.html</link><dc:creator>tj2</dc:creator><description>&lt;div&gt;&lt;p&gt;如有雷同，纯属巧合&lt;/p&gt;
&lt;p&gt;&lt;a href="http://muxuezi.github.io/posts/slott-2015073-2015-06-25-report.html"&gt;Read more…&lt;/a&gt; (1 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>Lottery</category><guid>http://muxuezi.github.io/posts/slott-2015073-2015-06-25-report.html</guid><pubDate>Fri, 26 Jun 2015 00:00:00 GMT</pubDate></item></channel></rss>